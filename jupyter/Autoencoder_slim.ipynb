{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import slim\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder_slim(object):\n",
    "    \n",
    "    def __init__(self, inputs_shape= (50, 50), isTraining= True):\n",
    "        self.END = {}\n",
    "        self.learning_rate = 0.01\n",
    "        self.isTraining = isTraining\n",
    "        self.inputs_shape = inputs_shape\n",
    "        self.inputs_size = int(np.prod(self.inputs_shape))\n",
    "        self.inputs = tf.placeholder(tf.float32, shape= [None] + list(self.inputs_shape), name= 'inputs')\n",
    "        self.param_conv = dict(kernel_size= 3, stride= 1, padding= 'same', activation_fn= tf.nn.relu)\n",
    "        self.END = self.network()\n",
    "        pass\n",
    "    \n",
    "    def network(self, num_layers= 1):\n",
    "        self.END['inputs'] = net = self.inputs\n",
    "        with tf.name_scope('input_conv'):\n",
    "            self.END['input_conv'] = net = slim.repeat(net, 2, slim.conv1d, num_outputs= 64, **self.param_conv)\n",
    "        with tf.name_scope('encoder'):\n",
    "            self.END['encoder'] = net = slim.repeat(net, 2, slim.conv1d, num_outputs= 128, **self.param_conv)\n",
    "        with tf.name_scope('code'):\n",
    "            self.END['code'] = net = slim.repeat(net, 2, slim.conv1d, num_outputs= 2, **self.param_conv)\n",
    "        #with tf.name_scope('noise'):\n",
    "        #    self.END['noise'] = tf.random_normal(net.get_shape().as_list())\n",
    "        #    self.END['z'] = net = net + self.END['noise']\n",
    "        with tf.name_scope('decoder'):\n",
    "            self.END['decoder'] = net = slim.repeat(net, 2, slim.conv1d, num_outputs= 64, **self.param_conv)\n",
    "        with tf.name_scope('output_conv'):\n",
    "            self.END['output_conv'] = net = slim.repeat(net, 2, slim.conv1d, num_outputs= list(self.inputs_shape)[-1], **self.param_conv)\n",
    "        self.END['outputs'] = net = tf.reshape(net, [-1] + list(self.inputs_shape), name= 'outputs')\n",
    "        if self.isTraining:\n",
    "            with tf.name_scope('loss'):\n",
    "                self.END['loss'] = loss = tf.reduce_sum(self.inputs * tf.log(self.END['outputs'] + 1e-9) + (1 - self.inputs) * tf.log((1 - self.END['outputs'] + 1e-9)))\n",
    "                tf.summary.scalar(\"Loss\", loss)\n",
    "            with tf.name_scope('optimizer'):\n",
    "                self.END['optimizer'] = optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(loss)\n",
    "        return self.END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(os.getcwd(), \"model\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "MODEL_NAME = \"CNN.ckpt\"\n",
    "LOG_DIR = \"TensorBoard/\"\n",
    "SAVE_PATH = os.path.join(model_dir, MODEL_NAME)\n",
    "Training = True\n",
    "\n",
    "Name = 'Bus'\n",
    "cur_dir = os.getcwd()\n",
    "dataset = os.path.join(cur_dir, 'dataset')\n",
    "dataset_dir = os.path.join(dataset, Name)\n",
    "subimages = os.path.join(dataset_dir, 'SubImages')\n",
    "images_path = []\n",
    "if os.path.exists(subimages):\n",
    "    images_name = os.listdir(subimages)\n",
    "    for name in images_name:\n",
    "        images_path.append(os.path.join(subimages, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Net\n",
    "inputs_shape = (50, 50)\n",
    "auto = autoencoder_slim(inputs_shape= inputs_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVER = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "cfg = tf.ConfigProto(log_device_placement= False, allow_soft_placement= True)\n",
    "cfg.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "cfg.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config= cfg)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 50)\n",
      "Loss : -20441.34765625\n",
      "(1, 50, 50)\n",
      "Loss : -22123.59375\n",
      "(1, 50, 50)\n",
      "Loss : -24951.7890625\n"
     ]
    }
   ],
   "source": [
    "inp = auto.inputs\n",
    "if Training:\n",
    "    img = cv2.imread(images_path[0])\n",
    "    img_shape = img.shape\n",
    "    img = cv2.resize(img, (inputs_shape[1], inputs_shape[0]), interpolation= cv2.INTER_LANCZOS4)\n",
    "    out_img = np.asarray(img)\n",
    "    for i in range(3):\n",
    "        inp_img = np.reshape(img[:, :,i], [1] + list(inputs_shape))\n",
    "        inp_img = inp_img / 255\n",
    "        sess.run(auto.END['optimizer'], feed_dict= {inp: inp_img})\n",
    "        loss = auto.END['loss'].eval(feed_dict= {inp: inp_img})\n",
    "        print (f\"Loss : {loss}\")\n",
    "        inp_conv = auto.END['input_conv'].eval(feed_dict= {inp: inp_img})\n",
    "        enc = auto.END['encoder'].eval(feed_dict= {inp: inp_img})\n",
    "        code = auto.END['code'].eval(feed_dict= {inp: inp_img})\n",
    "        dec = auto.END['decoder'].eval(feed_dict= {inp: inp_img})\n",
    "        oup_conv = auto.END['output_conv'].eval(feed_dict= {inp: inp_img})\n",
    "        out_img[:, :, i] = oup_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Conv : (1, 50, 64)\n",
      "Encoder : (1, 50, 128)\n",
      "Code : (1, 50, 2)\n",
      "Decoder : (1, 50, 64)\n",
      "Output Conv : (1, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "print (f\"Input Conv : {inp_conv.shape}\\nEncoder : {enc.shape}\\nCode : {code.shape}\\nDecoder : {dec.shape}\\nOutput Conv : {oup_conv.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_img = np.reshape(out_img * 255, list(inputs_shape) + [3])\n",
    "out_img = cv2.resize(out_img, (img_shape[1], img_shape[0]), interpolation= cv2.INTER_LANCZOS4).astype(np.uint8)\n",
    "plt.imshow(out_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : -20441.34765625\n",
      "Loss : -22123.59375\n",
      "Loss : -24951.7890625\n",
      "Loss : -20441.34765625\n",
      "Loss : -22123.59375\n",
      "Loss : -24951.7890625\n",
      "Loss : -20441.34765625\n",
      "Loss : -22123.59375\n",
      "Loss : -24951.7890625\n",
      "Loss : -20441.34765625\n",
      "Loss : -22123.59375\n",
      "Loss : -24951.7890625\n",
      "Loss : -20441.34765625\n",
      "Loss : -22123.59375\n",
      "Loss : -24951.7890625\n",
      "Loss : -20441.34765625\n",
      "Loss : -22123.59375\n",
      "Loss : -24951.7890625\n",
      "Loss : -20441.34765625\n",
      "Loss : -22123.59375\n",
      "Loss : -24951.7890625\n",
      "Loss : -20441.34765625\n",
      "Loss : -22123.59375\n",
      "Loss : -24951.7890625\n",
      "Loss : -20441.34765625\n",
      "Loss : -22123.59375\n",
      "Loss : -24951.7890625\n",
      "Loss : -20441.34765625\n",
      "Loss : -22123.59375\n",
      "Loss : -24951.7890625\n"
     ]
    }
   ],
   "source": [
    "inp = auto.inputs\n",
    "out_imgs = []\n",
    "losses = []\n",
    "codes = []\n",
    "if Training:\n",
    "    num_iterations = 1000\n",
    "    log_iterations = 100\n",
    "    for i in range(num_iterations):\n",
    "        img = cv2.imread(images_path[0])\n",
    "        img_shape = img.shape\n",
    "        img = cv2.resize(img, (inputs_shape[1], inputs_shape[0]), interpolation= cv2.INTER_LANCZOS4)\n",
    "        out_img = np.asarray(img)\n",
    "        for j in range(3):\n",
    "            inp_img = np.reshape(img[:, :,j], [1] + list(inputs_shape))\n",
    "            inp_img = inp_img / 255\n",
    "            sess.run(auto.END['optimizer'], feed_dict= {inp: inp_img})\n",
    "            if i % log_iterations == 0:\n",
    "                loss = auto.END['loss'].eval(feed_dict= {inp: inp_img})\n",
    "                print (f\"Loss : {loss}\")\n",
    "                code = auto.END['code'].eval(feed_dict= {inp: inp_img})\n",
    "                codes.append(code)                                      \n",
    "                losses.append(loss)\n",
    "            oup_conv = auto.END['output_conv'].eval(feed_dict= {inp: inp_img})\n",
    "        out_img[:, :, j] = oup_conv\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(LOG_DIR, graph = sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

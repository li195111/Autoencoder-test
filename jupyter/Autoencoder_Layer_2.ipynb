{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(object):\n",
    "    \n",
    "    def __init__(self, Training= True):\n",
    "        self.isTrain = Training\n",
    "        self.N_Pixels = 300*300\n",
    "        \n",
    "        self.Layer_1 = 50\n",
    "        \n",
    "        self.Latten_Dim = 2\n",
    "        \n",
    "        self.num_layer = 0\n",
    "        \n",
    "        self.ENDPOINT = {}\n",
    "        \n",
    "        self.two_layers()\n",
    "            \n",
    "    def weight_var(self, shape, name):\n",
    "        initial = tf.truncated_normal(shape= shape, stddev= 0.1)\n",
    "        return tf.Variable(initial, name= name)\n",
    "    \n",
    "    def bias_var(self, shape, name):\n",
    "        initial = tf.truncated_normal(shape= shape, stddev= 0.1)\n",
    "        return tf.Variable(initial, name= name)\n",
    "    \n",
    "    def full_connect_layer(self, x, w, b):\n",
    "        return tf.matmul(x, w) + b\n",
    "    \n",
    "    def Layer(self, shape, inputs, name):\n",
    "        self.num_layer += 1\n",
    "        W = self.weight_var(shape, 'w_' + name + '_' + str(self.num_layer))\n",
    "        B = self.bias_var([shape[1]], 'b_' + name + '_' + str(self.num_layer))\n",
    "        return self.full_connect_layer(inputs, W, B)\n",
    "    \n",
    "    def two_layers(self):\n",
    "        self.X = tf.placeholder(dtype= tf.float32, shape= [None, self.N_Pixels], name= 'X')\n",
    "        # Encoder Layer 1\n",
    "        self.ENDPOINT['enc_1'] = enc_1 = self.Layer([self.N_Pixels, self.Layer_1], self.X, 'enc')\n",
    "        # Activation\n",
    "        self.ENDPOINT['a_enc_1'] = a_enc_1 = tf.nn.tanh(enc_1)\n",
    "        # Encoder Layer 2\n",
    "        self.ENDPOINT['enc_2'] = enc_2 = self.Layer([self.Layer_1, self.Latten_Dim], a_enc_1, 'enc')\n",
    "        \n",
    "        self.ENDPOINT['logstd'] = logstd = self.Layer([self.Layer_1, self.Latten_Dim], a_enc_1, 'logstd')\n",
    "        \n",
    "        self.ENDPOINT['noise'] = noise = tf.random_normal([1, self.Latten_Dim])\n",
    "        \n",
    "        self.ENDPOINT['z'] = z = enc_2 + tf.multiply(noise, tf.exp(0.5 * logstd))\n",
    "        \n",
    "        # Decoder Layer 1\n",
    "        self.ENDPOINT['dec_1'] = dec_1 = self.Layer([self.Latten_Dim, self.Layer_1], z, 'dec')\n",
    "        # Activation\n",
    "        self.ENDPOINT['a_dec_2'] = a_dec_2 = tf.nn.relu(dec_1)\n",
    "        # Decoder Layer 2\n",
    "        self.ENDPOINT['dec_2'] = dec_2 = self.Layer([self.Layer_1, self.N_Pixels], a_dec_2, 'dec')\n",
    "        # Output Activation\n",
    "        self.ENDPOINT['a_dec_1'] = a_dec_1 = tf.nn.sigmoid(dec_2)\n",
    "        if self.isTrain:\n",
    "            self.log_likelihood = self.Log_Likelihood(self.X, self.ENDPOINT['a_dec_1'])\n",
    "            self.kl_divergence = self.KL_Divergence(self.ENDPOINT['enc_2'], self.ENDPOINT['logstd'])\n",
    "            self.variation_lower_bound = self.Variation_Lower_Bound(self.log_likelihood, self.kl_divergence)\n",
    "            tf.summary.scalar(\"Loss\", self.log_likelihood)\n",
    "            with tf.name_scope(\"Accuracy\"):\n",
    "                tf.summary.scalar(\"Accuracy\", self.variation_lower_bound)\n",
    "            self.optimizer = self.AOptimizer(self.variation_lower_bound)\n",
    "        return a_dec_1\n",
    "    \n",
    "    def Log_Likelihood(self, inputs, a_dec_1):\n",
    "        log_likelihood = tf.reduce_sum(inputs * tf.log(a_dec_1 + 1e-9) + (1 - inputs) * tf.log(1 - inputs + 1e-9), reduction_indices= 1)\n",
    "        return log_likelihood\n",
    "    \n",
    "    def KL_Divergence(self, mean, std):\n",
    "        kl_tern = -0.5 * tf.reduce_sum(1 + 2 * std - tf.pow(mean, 2) - tf.exp(2 * std), reduction_indices= 1)\n",
    "        return kl_tern\n",
    "    \n",
    "    def Variation_Lower_Bound(self, Log_Likelihood, KL_Divergence):\n",
    "        return tf.reduce_mean(Log_Likelihood + KL_Divergence)\n",
    "    \n",
    "    def Optimizer(self, loss):\n",
    "        optimizer = tf.train.AdadeltaOptimizer().minimize(loss)\n",
    "        return optimizer\n",
    "    \n",
    "    def AOptimizer(self, loss):\n",
    "        with tf.name_scope(\"Adagrad\"):\n",
    "            optimizer = tf.train.AdagradOptimizer(0.1).minimize(loss)\n",
    "        return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dir = os.path.join(os.getcwd(), \"model\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "MODEL_NAME = \"CNN.ckpt\"\n",
    "LOG_DIR = \"TensorBoard/\"\n",
    "SAVE_PATH = os.path.join(model_dir, MODEL_NAME)\n",
    "Training = True\n",
    "\n",
    "Name = 'Bus'\n",
    "cur_dir = os.getcwd()\n",
    "dataset = os.path.join(cur_dir, 'dataset')\n",
    "dataset_dir = os.path.join(dataset, Name)\n",
    "subimages = os.path.join(dataset_dir, 'SubImages')\n",
    "images_path = []\n",
    "if os.path.exists(subimages):\n",
    "    images_name = os.listdir(subimages)\n",
    "    for name in images_name:\n",
    "        images_path.append(os.path.join(subimages, name))\n",
    "\n",
    "auto = autoencoder(Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVER = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "cfg = tf.ConfigProto(log_device_placement= False, allow_soft_placement= True)\n",
    "cfg.gpu_options.per_process_gpu_memory_fraction = 0.90\n",
    "cfg.gpu_options.allow_growth = True\n",
    "cfg.gpu_options.allocator_type ='BFC'\n",
    "sess = tf.InteractiveSession(config= cfg)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(LOG_DIR, graph = sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = auto.optimizer\n",
    "variational_lower_bound = auto.variation_lower_bound\n",
    "log_likelihood = auto.log_likelihood\n",
    "KL_tern = auto.kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Training:\n",
    "    models = os.listdir(MODEL_SAVE_PATH)\n",
    "    for file in models:\n",
    "        if '.ckpt' in file:\n",
    "            SAVER.restore(sess, SAVE_PATH)\n",
    "            break\n",
    "    img = cv2.imread(images_path[0])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_shape = img.shape\n",
    "    img = cv2.resize(img, (300, 300), interpolation= cv2.INTER_LANCZOS4)\n",
    "    img = np.reshape(img, [1, img.size])\n",
    "    img = img / 255\n",
    "    X = auto.X\n",
    "    code = auto.ENDPOINT['logstd'].eval(feed_dict = {X: img})\n",
    "    show_img = auto.ENDPOINT['a_dec_1'].eval(feed_dict = {X: img})\n",
    "    show_img *= 255\n",
    "    show_img = np.reshape(show_img, (300, 300))\n",
    "    show_img = cv2.resize(show_img, (img_shape[1], img_shape[0]), interpolation= cv2.INTER_LANCZOS4)\n",
    "    plt.imshow(show_img, cmap= 'gray')\n",
    "    plt.show()\n",
    "    print (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images Length : 432 \n",
      "Total Interations : 432\n",
      "Interval : 4\n",
      "Iteration : 0, Loss : -40901.765625\n",
      "Code : [[3.9916995 4.973943 ]]\n",
      "Iteration : 4, Loss : -964647.3125\n",
      "Code : [[-4.341372    0.54623634]]\n",
      "Iteration : 8, Loss : -910697.25\n",
      "Code : [[-4.3849287  0.9807164]]\n",
      "Iteration : 12, Loss : -1252318.75\n",
      "Code : [[-4.3840036  0.9805078]]\n",
      "Iteration : 16, Loss : -1075904.75\n",
      "Code : [[-4.364463  1.142786]]\n",
      "Iteration : 20, Loss : -734371.75\n",
      "Code : [[-4.372958   0.7141361]]\n",
      "Iteration : 24, Loss : -979737.125\n",
      "Code : [[-4.372049   0.7140163]]\n",
      "Iteration : 28, Loss : -767053.0625\n",
      "Code : [[-4.3711486   0.71389353]]\n",
      "Iteration : 32, Loss : -553687.5625\n",
      "Code : [[-4.155328    0.31772453]]\n",
      "Iteration : 36, Loss : -1368376.375\n",
      "Code : [[-4.154436    0.31761605]]\n",
      "Iteration : 40, Loss : -1010307.1875\n",
      "Code : [[-4.1534843   0.31757995]]\n",
      "Iteration : 44, Loss : -660918.25\n",
      "Code : [[-4.1525617   0.31754726]]\n",
      "Iteration : 48, Loss : -540537.0\n",
      "Code : [[-4.151634   0.3174643]]\n",
      "Iteration : 52, Loss : -791793.5625\n",
      "Code : [[-4.097307  -0.1404672]]\n",
      "Iteration : 56, Loss : -971068.4375\n",
      "Code : [[-4.096389   -0.14045765]]\n",
      "Iteration : 60, Loss : -672735.25\n",
      "Code : [[-4.095498   -0.14045697]]\n",
      "Iteration : 64, Loss : -821365.875\n",
      "Code : [[-4.0057507 -0.3996396]]\n",
      "Iteration : 68, Loss : -798620.875\n",
      "Code : [[-4.00493    -0.39967078]]\n",
      "Iteration : 72, Loss : -803975.4375\n",
      "Code : [[-4.004155  -0.3996861]]\n",
      "Iteration : 76, Loss : -952565.625\n",
      "Code : [[-4.003284   -0.39967066]]\n",
      "Iteration : 80, Loss : -697483.75\n",
      "Code : [[-4.0902386 -0.7425016]]\n",
      "Iteration : 84, Loss : -841646.0\n",
      "Code : [[-4.089133  -0.7424765]]\n",
      "Iteration : 88, Loss : -1064758.75\n",
      "Code : [[-4.088347  -0.7424842]]\n",
      "Iteration : 92, Loss : -603664.8125\n",
      "Code : [[-4.087508   -0.74255776]]\n",
      "Iteration : 96, Loss : -953047.8125\n",
      "Code : [[-4.086653   -0.74255514]]\n",
      "Iteration : 100, Loss : -1126399.75\n",
      "Code : [[-4.0856996 -0.742543 ]]\n",
      "Iteration : 104, Loss : -815183.75\n",
      "Code : [[-4.0847607  -0.74251956]]\n",
      "Iteration : 108, Loss : -693915.8125\n",
      "Code : [[-4.0839005 -0.7425038]]\n",
      "Iteration : 112, Loss : -813178.875\n",
      "Code : [[-4.0829625 -0.7424783]]\n",
      "Iteration : 116, Loss : -1255297.125\n",
      "Code : [[-4.081936   -0.74248147]]\n",
      "Iteration : 120, Loss : -766826.5625\n",
      "Code : [[-4.081      -0.74245685]]\n",
      "Iteration : 124, Loss : -885738.375\n",
      "Code : [[-4.080072  -0.7424331]]\n",
      "Iteration : 128, Loss : -783427.75\n",
      "Code : [[-4.079126  -0.7424206]]\n",
      "Iteration : 132, Loss : -995651.625\n",
      "Code : [[-4.0782495 -0.7423978]]\n",
      "Iteration : 136, Loss : -707087.0625\n",
      "Code : [[-4.077357  -0.7423723]]\n",
      "Iteration : 140, Loss : -967975.875\n",
      "Code : [[-4.076471   -0.74234706]]\n",
      "Iteration : 144, Loss : -1040449.625\n",
      "Code : [[-4.0756874  -0.74234605]]\n",
      "Iteration : 148, Loss : -570979.3125\n",
      "Code : [[-3.777955   -0.87975127]]\n",
      "Iteration : 152, Loss : -560704.8125\n",
      "Code : [[-3.7770326 -0.8797346]]\n",
      "Iteration : 156, Loss : -1122253.125\n",
      "Code : [[-3.7761168  -0.87970257]]\n",
      "Iteration : 160, Loss : -966474.125\n",
      "Code : [[-3.7750764  -0.87972355]]\n",
      "Iteration : 164, Loss : -1014418.125\n",
      "Code : [[-3.7741566 -0.879692 ]]\n",
      "Iteration : 168, Loss : -837709.6875\n",
      "Code : [[-3.7731404 -0.8796795]]\n",
      "Iteration : 172, Loss : -1041647.1875\n",
      "Code : [[-3.7724063  -0.87970245]]\n",
      "Iteration : 176, Loss : -502635.6875\n",
      "Code : [[-3.7714758 -0.8796744]]\n",
      "Iteration : 180, Loss : -910037.9375\n",
      "Code : [[-3.7704105  -0.87965953]]\n",
      "Iteration : 184, Loss : -917750.5\n",
      "Code : [[-3.7694764 -0.879639 ]]\n",
      "Iteration : 188, Loss : -781872.9375\n",
      "Code : [[-3.7685766  -0.87961555]]\n",
      "Iteration : 192, Loss : -659959.75\n",
      "Code : [[-3.7677836  -0.87959415]]\n",
      "Iteration : 196, Loss : -799564.4375\n",
      "Code : [[-3.7668278 -0.8795779]]\n",
      "Iteration : 200, Loss : -853805.3125\n",
      "Code : [[-3.76616   -0.8795572]]\n",
      "Iteration : 204, Loss : -1151158.375\n",
      "Code : [[-3.7653084  -0.87953603]]\n",
      "Iteration : 208, Loss : -777568.875\n",
      "Code : [[-3.7643874  -0.87950456]]\n",
      "Iteration : 212, Loss : -876944.9375\n",
      "Code : [[-3.7635417 -0.8794785]]\n",
      "Iteration : 216, Loss : -752604.625\n",
      "Code : [[-3.762789  -0.8794565]]\n",
      "Iteration : 220, Loss : -1037383.75\n",
      "Code : [[-3.7619474 -0.8794273]]\n",
      "Iteration : 224, Loss : -342383.6875\n",
      "Code : [[-3.7611644 -0.8794153]]\n",
      "Iteration : 228, Loss : -1281339.5\n",
      "Code : [[-3.7602232  -0.87938523]]\n",
      "Iteration : 232, Loss : -644651.8125\n",
      "Code : [[-3.7592242  -0.87935925]]\n",
      "Iteration : 236, Loss : -682387.375\n",
      "Code : [[-3.7583084 -0.8793298]]\n",
      "Iteration : 240, Loss : -762754.5\n",
      "Code : [[-3.7575767 -0.8793074]]\n",
      "Iteration : 244, Loss : -775675.0625\n",
      "Code : [[-3.756661  -0.8792814]]\n",
      "Iteration : 248, Loss : -859561.6875\n",
      "Code : [[-3.755995  -0.8792803]]\n",
      "Iteration : 252, Loss : -1003633.0\n",
      "Code : [[-3.755283   -0.87927043]]\n",
      "Iteration : 256, Loss : -837236.625\n",
      "Code : [[-3.7543359 -0.8792527]]\n",
      "Iteration : 260, Loss : -1053218.75\n",
      "Code : [[-3.7533422 -0.8792292]]\n",
      "Iteration : 264, Loss : -1087635.25\n",
      "Code : [[-3.7524552  -0.87920916]]\n",
      "Iteration : 268, Loss : -1155954.75\n",
      "Code : [[-3.7516851 -0.8792081]]\n",
      "Iteration : 272, Loss : -820986.6875\n",
      "Code : [[-3.7509234 -0.8792495]]\n",
      "Iteration : 276, Loss : -1014739.3125\n",
      "Code : [[-3.749938   -0.87924635]]\n",
      "Iteration : 280, Loss : -1142471.125\n",
      "Code : [[-3.7489998  -0.87921965]]\n",
      "Iteration : 284, Loss : -491957.1875\n",
      "Code : [[-3.7480724 -0.8791901]]\n",
      "Iteration : 288, Loss : -649136.0625\n",
      "Code : [[-3.7471473 -0.8791638]]\n",
      "Iteration : 292, Loss : -658433.5\n",
      "Code : [[-3.7462342  -0.87913644]]\n",
      "Iteration : 296, Loss : -728136.5\n",
      "Code : [[-3.7453158  -0.87910736]]\n",
      "Iteration : 300, Loss : -629636.3125\n",
      "Code : [[-3.7443516  -0.87908393]]\n",
      "Iteration : 304, Loss : -894304.3125\n",
      "Code : [[-3.743435  -0.8790545]]\n",
      "Iteration : 308, Loss : -1195912.5\n",
      "Code : [[-3.7425635 -0.8790281]]\n",
      "Iteration : 312, Loss : -1117989.125\n",
      "Code : [[-3.7416506 -0.8789992]]\n",
      "Iteration : 316, Loss : -583028.1875\n",
      "Code : [[-3.74076   -0.8789724]]\n",
      "Iteration : 320, Loss : -848839.9375\n",
      "Code : [[-3.7398384 -0.8789504]]\n",
      "Iteration : 324, Loss : -734615.9375\n",
      "Code : [[-3.7389238  -0.87892085]]\n",
      "Iteration : 328, Loss : -743068.375\n",
      "Code : [[-3.738013   -0.87889165]]\n",
      "Iteration : 332, Loss : -497199.375\n",
      "Code : [[-3.6821365  -0.51088065]]\n",
      "Iteration : 336, Loss : -1062306.0\n",
      "Code : [[-3.6810849 -0.5108693]]\n",
      "Iteration : 340, Loss : -908635.125\n",
      "Code : [[-3.6788368  -0.51142514]]\n",
      "Iteration : 344, Loss : -643985.625\n",
      "Code : [[-3.677865  -0.5113931]]\n",
      "Iteration : 348, Loss : -568868.3125\n",
      "Code : [[-3.677707  -0.5114245]]\n",
      "Iteration : 352, Loss : -1072181.75\n",
      "Code : [[-3.6770551  -0.51146716]]\n",
      "Iteration : 356, Loss : -753930.5\n",
      "Code : [[-3.6761014 -0.5114397]]\n",
      "Iteration : 360, Loss : -818263.9375\n",
      "Code : [[-3.675351  -0.5115299]]\n",
      "Iteration : 364, Loss : -765684.625\n",
      "Code : [[-3.6743476  -0.51151824]]\n",
      "Iteration : 368, Loss : -839892.9375\n",
      "Code : [[-3.6735675  -0.51150906]]\n",
      "Iteration : 372, Loss : -804094.75\n",
      "Code : [[-3.67266  -0.511501]]\n",
      "Iteration : 376, Loss : -751727.6875\n",
      "Code : [[-3.6716883 -0.5115174]]\n",
      "Iteration : 380, Loss : -749730.0625\n",
      "Code : [[-3.670766   -0.51148957]]\n",
      "Iteration : 384, Loss : -769540.5625\n",
      "Code : [[-3.6699347  -0.51147145]]\n",
      "Iteration : 388, Loss : -1012641.875\n",
      "Code : [[-3.6689992 -0.5114627]]\n",
      "Iteration : 392, Loss : -627617.25\n",
      "Code : [[-3.6680262 -0.5114595]]\n",
      "Iteration : 396, Loss : -889068.5\n",
      "Code : [[-3.6670332  -0.51143706]]\n",
      "Iteration : 400, Loss : -835467.3125\n",
      "Code : [[-3.6660023 -0.5114327]]\n",
      "Iteration : 404, Loss : -913353.25\n",
      "Code : [[-3.665033   -0.51142347]]\n",
      "Iteration : 408, Loss : -1134625.25\n",
      "Code : [[-3.664118 -0.511405]]\n",
      "Iteration : 412, Loss : -774037.3125\n",
      "Code : [[-3.6635175 -0.5114449]]\n",
      "Iteration : 416, Loss : -748958.75\n",
      "Code : [[-3.6628726 -0.5114498]]\n",
      "Iteration : 420, Loss : -635286.75\n",
      "Code : [[-3.6619916  -0.51144177]]\n",
      "Iteration : 424, Loss : -804178.625\n",
      "Code : [[-3.6610446  -0.51141965]]\n",
      "Iteration : 428, Loss : -826547.25\n",
      "Code : [[-3.6599212 -0.5114793]]\n"
     ]
    }
   ],
   "source": [
    "num_iterations = len(images_path)\n",
    "recording_interval = 4\n",
    "variational_lower_bound_array = []\n",
    "log_likelihood_array = []\n",
    "KL_tern_array = []\n",
    "interation_array = [i * recording_interval for i in range(int(num_iterations / recording_interval))]\n",
    "print (f\"Images Length : {len(images_path)} \\nTotal Interations : {num_iterations}\\nInterval : {recording_interval}\")\n",
    "for i in range(num_iterations):\n",
    "    img = cv2.imread(images_path[i])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_shape = img.shape\n",
    "    img = cv2.resize(img, (300, 300), interpolation= cv2.INTER_LANCZOS4)\n",
    "    img = np.reshape(img, [1, img.size])\n",
    "    img = img / 255\n",
    "    X = auto.X\n",
    "    sess.run(auto.optimizer, feed_dict = {X: img})\n",
    "    code = auto.ENDPOINT['logstd'].eval(feed_dict = {X: img})\n",
    "    if (i % recording_interval == 0):\n",
    "        vlb_eval = variational_lower_bound.eval(feed_dict = {X: img})\n",
    "        print (\"Iteration : {}, Loss : {}\".format(i, vlb_eval))\n",
    "        print (f\"Code : {code}\")\n",
    "        variational_lower_bound_array.append(vlb_eval)\n",
    "        log_likelihood_array.append(np.mean(log_likelihood.eval(feed_dict = {X: img})))\n",
    "        KL_tern_array.append(np.mean(KL_tern.eval(feed_dict = {X: img})))\n",
    "        #show_img = auto.ENDPOINT['a_dec_1'].eval(feed_dict = {X: img})\n",
    "        #show_img *= 255\n",
    "        #show_img = np.reshape(show_img, (300, 300))\n",
    "        #show_img = cv2.resize(show_img, (img_shape[1], img_shape[0]), interpolation= cv2.INTER_LANCZOS4)\n",
    "        #plt.imshow(show_img, cmap= 'gray')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVER.save(sess, SAVE_PATH)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print (auto.isTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
